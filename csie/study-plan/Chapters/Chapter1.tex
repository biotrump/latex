% Chapter 1
\chapter{Fundamental Mathematics} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1}

\lhead{Chapter 1. \emph{Fundamental Mathematics}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
\section{Linear algebra}
\begin{compactitem}

\item {Diagonal Matrix Properties}
In linear algebra, a diagonal matrix is a matrix (usually a square matrix) in which
the entries outside the main diagonal are all zero. The diagonal entries themselves
may or may not be zero. Thus, the matrix $D = (d_{i,j})$ with n columns and n rows is diagonal
if:
$d_{i,j}=0,\, if \,\, i \neq j \,\, \forall i,j \in \{1,2,3,...n\} $
\\For example, the following matrix is diagonal:\\
$\begin{pmatrix}
       1	&0 	&0	\\[0.3em]
       0 	&4 	&0 	\\[0.3em]
       0 	& 0 	&-2	\\[0.3em]
\end{pmatrix}
$ \cite{wiki-Diagonal_matrix}
\\
\\The general form :$D=diag(\lambda_{ii}) =$
\begin{equation}
\label{eq:diagm}
\begin{pmatrix}
       \lambda_{00}& 0 				& ..	&	0 	\\[0.3em]
       0 			& \lambda_{11} & ..		&	0 	\\[0.3em]
       .			& .				& ..	&	.	\\[0.3em]
       0 			& 0 			& ..	&	\lambda_{nn}\\[0.3em]
     \end{pmatrix}
\end{equation}

\begin{equation}
\label{eq:pdpD}
D^{-1}=diag(\lambda_{ii}^{-1})=
\begin{pmatrix}
       1/\lambda_{00}& 0 				& ..	&	0 	\\[0.3em]
       0 			& 1/\lambda_{11} & ..		&	0 	\\[0.3em]
       .			& .				& ..	&	.	\\[0.3em]
       0 			& 0 			& ..	&	1/\lambda_{nn}\\[0.3em]
\end{pmatrix}
, \forall \Lambda_{ii} \neq 0,
\end{equation}\\

The determinant of a Diagonal matrix is the product of all the diagonal elements.
\begin{equation}
\label{eq:detD}
det(D) = \prod_{i}^{} \lambda_{ii}
\end{equation}

\begin{equation}
\label{eq:det-pdpD}
\\
det(D^{-1})=
\prod_{i}^{} \frac{1}{\lambda_{ii} }\\
=\prod_{i}^{} \lambda_{ii}^{-1} \\
\end{equation}\\

%----------------------------------------------------------------------------------------
\item {Eigen Values and Eigen Vectors}
\\If A is a n*n square matrix. Eigen value $\lambda$ and \textbf{eigen} vector $\hat{x}$: \cite{AntonELA10th}
\begin{equation}
\label{eq:eval}
A\hat{x}=\lambda \hat{x}
\end{equation}
The sign of $\lambda$ and $\hat{x}$ are not unique. And $\lambda$ and $\hat{x}$ are scalable.
\\$ (A-\lambda I)X=0$
Set the determinant to zero to obtain the polynomial equation to solve the eigen values.
$det(A-\lambda I)=0$

\item singular value decomposition
\\U and V are orthogonal matrix, $U^{-1}=U^{\top}, V^{-1}=V^{\top}$\\
r=rank A
\begin{flalign}
\underbrace{\mathbf{A}}_{M \times N} = \underbrace{\mathbf{U}}_{M \times M} \times
\underbrace{\mathbf{\Sigma}}_{M\times N} \times
\underbrace{\mathbf{V}^{\text{T}}}_{N \times N} = &
\end{flalign}

\hspace*{-3cm}\vbox{
\begin{flalign}
\begin{tikzpicture}[
baseline,
mymat/.style={
  matrix of math nodes,
  ampersand replacement=\&,
  left delimiter=(,
  right delimiter=),
  nodes in empty cells,
  nodes={outer sep=-\pgflinewidth,text depth=0.5ex,text height=2ex,text width=1.2em}
}
]
\begin{scope}[every right delimiter/.style={xshift=-3ex}]
\matrix[mymat] (matu)
{
 \& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
};
\node
  at ([shift={(3pt,-7pt)}]matu-3-2.west)
  {$\cdots$};
\node
  at ([shift={(3pt,-7pt)}]matu-3-5.west)
  {$\cdots$};
\foreach \Columna/\Valor in {1/1,3/r,4/{r+1},6/m}
{
\draw
  (matu-1-\Columna.north west)
    rectangle
  ([xshift=4pt]matu-6-\Columna.south west);
\node[above]
  at ([xshift=2pt]matu-1-\Columna.north west)
  {$u_{\Valor}$};
}
\draw[decorate,decoration={brace,mirror,raise=3pt}]
  (matu-6-1.south west) --
   node[below=4pt] {$\Mcol(A)$}
  ([xshift=4pt]matu-6-3.south west);
\draw[decorate,decoration={brace,mirror,raise=3pt}]
  (matu-6-4.south west) --
   node[below=4pt] {$\Mnull(A)$}
  ([xshift=4pt]matu-6-6.south west);
\end{scope}
\matrix[mymat,right=10pt of matu] (matsigma)
{
\sigma_{1} \& \& \& \& \& \\
\& \ddots \& \& \& \& \\
\& \& \sigma_{r} \& \& \& \\
\& \& \& 0 \& \& \\
\& \& \& \& \ddots \& \\
\& \& \& \& \& 0 \\
};
%\begin{scope}[every right delimiter/.style={xshift=-3ex}]
\matrix[mymat,right=25pt of matsigma] (matv)
{
 \& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
\& \& \& \& \& \\
};
\foreach \Fila/\Valor in {1/1,3/r,4/{r+1},6/n}
{
\draw
  ([yshift=-6pt]matv-\Fila-1.north west)
    rectangle
  ([yshift=-10pt]matv-\Fila-6.north east);
\node[right=12pt]
  at ([yshift=-8pt]matv-\Fila-6.north east)
  {$v^{T}_{\Valor}$};
}
\draw[decorate,decoration={brace,raise=37pt}]
  ([yshift=-6pt]matv-1-6.north east) --
   node[right=38pt] {$\Mrow(A)$}
  ([yshift=-10pt]matv-3-6.north east);
\draw[decorate,decoration={brace,raise=37pt}]
  ([yshift=-6pt]matv-4-6.north east) --
   node[right=38pt] {$\Mnull(A)$}
  ([yshift=-10pt]matv-6-6.north east);
\end{tikzpicture}
\end{flalign}}

%----------------------------------------------------------------------------------------
\item {Symmetric Matrix:}\\
A symmetric matrix is a square matrix that is equal to its transpose.
Formally, matrix A is symmetric if $A = A^{\top}$.
So if the entries are written as $A = (a_{ij}), \,then\,\, a_{ij} = a_{ji}$.
\\
A real sqaure symmetric matrix can be Eigen Value Decomposition : $A=PDP^{\top}$.
D is a diagonal matrix whose elements are eigen values of A.
P is composed of orthogonal eigen vectors, so $P^{-1}=P^{\top}$.\\
\begin{equation}
\label{eq:pdpI}
I=P* P^{-1} = P* P^{\top}
\end{equation}

\item positive definite matrix : $x^{\top} A x \geq 0 ,\forall\, x \, \in \, \mathbb{R}^{N}$
\\
Every positive definite matrix has positive eigenvalues.


\end{compactitem}
%----------------------------------------------------------------------------------------

\section{Covariance and Whitening}
\begin{compactitem}

\item {standard deviation and variance}
\begin{equation}
\label{eq:std}
\sigma = \sqrt {\sum \frac{(x-\bar{x})^2}{N}}
\end{equation}
\begin{equation}
\label{eq:variance}
VAR(X)=\sigma^{2} = {\sum \frac{(x-\bar{x})^2}{N}}
\end{equation}

\item covariance: X and Y are two vectors, random variables. The covariance is a value.
\begin{equation}
\label{eq:covar}
Cov(X,Y)=\sigma(X,Y)=E[(X - E[X])(Y-E[Y])] \\
Cov(X,Y)=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{N}=
\frac{\sum(x_i)(y_i)}{N}
\end{equation}

Deviation score $x_i, y_i$:
\begin{equation}
\label{eq:covar1}
x_{i}=(X_i-\bar{X}),\;
y_{i}=(Y_i-\bar{Y})
\end{equation}
Variance of X, $VAR(X)=Cov(X,X)=\sigma^{2}$, \ is a degenerated form of covariance.\\

\item {covariance matrix:}
is a measure of the extent to which corresponding elements from two sets
of ordered data move in the same direction. We use the following formula to compute covariance.
\cite{wiki-covariance}\cite{STAT-covariance}\\
\\Let $X_i$ be the \textbf{column} vector in a Matrix \textbf{X}=
$\begin{pmatrix} X_1 & X_2 &... & X_c \end{pmatrix}$\\
The covariance matrix of \textbf{X} is:\\
$M_{cxc} = \Sigma{(\textbf{X})}=$
\[
\begin{pmatrix}
       COV(X_1,X_1) 	& COV(X_1,X_2) 	& ...	& COV(X_1,X_c)	\\[0.3em]
       COV(X_2,X_1) 	& COV(X_2,X_2) 	& ...	& COV(X_2,X_c)	\\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       COV(X_c,X_1) 	& COV(X_c,X_2) 	& ...	& COV(X_c,X_c)	\\[0.3em]
\end{pmatrix}
=\]
\begin{equation}
\label{eq:covarm1}
\begin{pmatrix}
       \sum x_{1i}^{2}/N 	& \sum x_{1i} x_{2i}/N 	& ...	& \sum x_{1i} x_{ci}/N	\\[0.3em]
       \sum x_{2i} x_{1i}/N 	& \sum x_{2i} x_{2i}/N 	& ...	& \sum x_{2i} x_{ci}/N	\\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       \sum x_{ci} x_{1i}/N 	& \sum x_{ci} x_{2i}/N 	& ...	& \sum x_{ci}^{2}/N	\\[0.3em]
\end{pmatrix}
=(1/N)
\begin{pmatrix}
       \sum x_{1i}^{2} 	& \sum x_{1i} x_{2i} 	& ...	& \sum x_{1i} x_{ci}	\\[0.3em]
       \sum x_{2i} x_{1i} 	& \sum x_{2i} x_{2i} 	& ...	& \sum x_{2i} x_{ci}	\\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       \sum x_{ci} x_{1i} 	& \sum x_{ci} x_{2i} 	& ...	& \sum x_{ci}^{2}	\\[0.3em]
\end{pmatrix}
\end{equation}
where\\
N is the dim of the column vector $X_i$.\\
$x_i$ is a deviation score from the ith data set.\\
$\sum x_i^2 / N$ is the variance of elements from the ith data set.\\
$\sum x_i x_j / N$ is the covariance for elements from the ith and jth data sets.\\

\item Properties of Covariance matrix :
\textbf{square, symmetric and positive semidefinite}: \\
Eigen value decomposition for a \textbf{square, symmetric} matrix:
\begin{equation}
\label{eq:covd}
\Sigma(\textbf{X}) = U\Lambda U^T
\end{equation}


When A is positive semi-definite, $x^T A x >= 0$ and all eigen values are positive.  $\Sigma(\textbf{X})$ has all positive eigen values.

Diagonal matrix $\Lambda$ has all positive eigen values in its diagonal elements. $\Lambda ^ {1/2}$ doest exist and $\Lambda ^ {-1/2}$ doest exist, too.\\

\textbf{linearity of expectation matrix}: Let \textbf{X} be a random vector with covariance matrix
$\Sigma(\textbf{X})$, and let A be a matrix that can act on \textbf{X}. The covariance matrix of the vector A\textbf{X} is:
\begin{equation}
\label{eq:covlinear}
\Sigma(A\textbf{X}) = A\Sigma(\textbf{X})A^{T}
\end{equation}

\item whiten: Refer to \eqref{eq:covd},  A \textbf{whitening} matrix W is defined as
\begin{equation}
\label{eq:whitem}
W=U\Lambda^{-1/2} U^{T}
\end{equation}
This W is not unique, because $\Lambda$ and $U^T$ are not unique.
When the diagonal elements, eigen value, of the diagonal matrix, $\Lambda$, are changed in the order.
$U$ is also changed .

Let X and Y are \textbf{row-major} matrix, i.e, signals are in row vectors.
\begin{equation}
\label{eq:whitened}
Y_{nm} = W_{nn}*\textbf{X}_{nm}= (U\Lambda^{-1/2} U^{T}) \textbf{X}_{nm}
\end{equation}
Sometimes, column-major is used in fortran and some math tools,
\begin{equation}
\label{eq:whitened-col}
Y' = (W_{nn}*\textbf{X}_{nm})' = \textbf{X}'*W'= \textbf{X}'*(U\Lambda^{-1/2} U^{T})'=\textbf{X}'*(U^{T}\Lambda^{-1/2} U)
\end{equation}
be a \textbf{whitened matrix} of \textbf{X}.\\
The covariance matrix of Y which is a whitened matrix of X.
\[
\Sigma (Y)=\Sigma (U\Lambda^{-1/2} U^{T}) \textbf{X}
\]
Refer to the equation \eqref{eq:covlinear}
\[
A=(U\Lambda^{-1/2} U^{T})
\]
so
\[
\Sigma (Y)=\Sigma (U\Lambda^{-1/2} U^{T}) \textbf{X}=
(U\Lambda^{-1/2} U^{T})(\Sigma (\textbf{X})) (U\Lambda^{-1/2} U^{T})^{T}
\]
Refer to the equation \eqref{eq:covd}\\
$=(U\Lambda^{-1/2} U^{T}) (U \Lambda U^{T}) (U\Lambda^{-1/2} U^{T}) ^T$\\
$=U\Lambda^{-1/2} (U^{T}  U) \Lambda U^{T} (U\Lambda^{-1/2} U^{T}) ^T$\\
Refer to the equation \eqref{eq:pdpI}\\
$=U\Lambda^{-1/2} (I) \Lambda U^{T} ({U^{T}}^T{\Lambda^{-1/2}}^T U^T)$\\
$=U\Lambda^{-1/2} \Lambda U^{T} (U{\Lambda^{-1/2}}^T) U^T$\\
$=U\Lambda^{-1/2} \Lambda (U^{T} U) {\Lambda^{-1/2}}^T U^T$\\
$=U\Lambda^{-1/2} \Lambda (I) {\Lambda^{-1/2}}^T U^T$\\
$=U\Lambda^{-1/2} \Lambda \Lambda^{-1/2} U^T$\\
$=U\Lambda^{(-1/2) + (1) + (-1/2)} U^T$\\
$=U I U^T = U U^T = I $\\
So \textbf{Y is whitened}, because its covariance matrix is an Identity matrix.
All row vectors in Y has no correlation!

\item \textbf{Simplification} for a \textbf{normalized} vector $\hat{X_i}$:\\
Let \textbf{X}=$\begin{pmatrix} X_1 & X_2 & ... & X_c \end{pmatrix}, X_1,X_2,X_c$ are column vectors which have been normalized.
(zero mean )\\

Refer to the equation \eqref{eq:covar1}, Deviation score $x_k, y_k$:\\
$x_{k}=(X_{ik}-\bar{X_i})=(X_{ik} - 0)= X_{ik}$ \\
$y_{k}=(Y_{ik}-\bar{Y_i})=(Y_{ik} - 0) = Y_{ik}$


Refer to the equation \eqref{eq:covarm1}, covariance matrix $\Sigma(\textbf{X})$=
\[
(1/N)
\begin{pmatrix}
       \sum x_{1k}^{2} 	& \sum x_{1k} x_{2k} 	& ...	& \sum x_{1k} x_{ck}	\\[0.3em]
       \sum x_{2k} x_{1k} 	& \sum x_{2k} x_{2k} 	& ...	& \sum x_{2k} x_{ck}	\\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       \sum x_{ck} x_{1k} 	& \sum x_{ck} x_{2k} 	& ...	& \sum x_{ck}^{2}	\\[0.3em]
\end{pmatrix}
\]
\[
=(1/N)
\begin{pmatrix}
       \sum X_{1k}^{2} 	& \sum X_{1k} X_{2k} 	& ...	& \sum X_{1k} X_{ck}	\\[0.3em]
       \sum X_{2k} X_{1k} 	& \sum X_{2k} X_{2k} 	& ...	& \sum X_{2k} X_{ck}	\\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       \sum X_{ck} X_{1k} 	& \sum X_{ck} X_{2k} 	& ...	& \sum X_{ck}^{2}	\\[0.3em]
\end{pmatrix}
\]
\[
=(1/N)
\begin{pmatrix}
       \langle X_1, X_1 \rangle	& \langle X_1, X_2 \rangle 	& ...	& \langle X_1, X_c	\rangle \\[0.3em]
       \langle X_2, X_1 \rangle 	& \langle X_2, X_2 \rangle	& ...	& \langle X_2, X_c	\rangle \\[0.3em]
       ...		& ...			& ...	&	...		\\[0.3em]
       \langle X_c, X_1 \rangle	& \langle X_c, X_2 \rangle 	& ...	& \langle X_c, X_c\rangle	\\[0.3em]
\end{pmatrix}
\]
Let \textbf{X}=$\begin{pmatrix} R & G & B \end{pmatrix}$, R,G,B are column vectors which have been normalized (detrend, zero mean).
\[
R=
\begin{pmatrix}
       R_0\\[0.3em]
       R_1\\[0.3em]
       R_2\\[0.3em]
       R_3\\[0.3em]
		. \\[0.3em]
       R_{n-1}\\[0.3em]
\end{pmatrix}
, G=
\begin{pmatrix}
       G_0\\[0.3em]
       G_1\\[0.3em]
       G_2\\[0.3em]
       G_3\\[0.3em]
		. \\[0.3em]
       G_{n-1}\\[0.3em]
\end{pmatrix}
, B=
\begin{pmatrix}
       B_0\\[0.3em]
       B_1\\[0.3em]
       B_2\\[0.3em]
       B_3\\[0.3em]
		. \\[0.3em]
       B_{n-1}\\[0.3em]
\end{pmatrix}
\]

\[
\Sigma(\textbf{X})=(1/N)
\begin{pmatrix}
       \langle R, R \rangle	& \langle R, G \rangle 	& \langle R, B	\rangle \\[0.3em]
       \langle G, R \rangle 	& \langle G, G \rangle		& \langle G, B	\rangle \\[0.3em]
       \langle B, R \rangle	& \langle B, G \rangle 	& \langle B, B	\rangle	\\[0.3em]
\end{pmatrix}
=
\begin{pmatrix}
       \sigma_R^2              & \langle R, G \rangle 	&  \langle R, B	\rangle \\[0.3em]
       \langle G, R \rangle 	& \sigma_G^2	            & \langle G, B	\rangle \\[0.3em]
       \langle B, R \rangle	& \langle B, G \rangle 	& \sigma_B^2	\\[0.3em]
\end{pmatrix}
\]
\[
=
\begin{pmatrix}
       R_0 & R_1 & R_2 & R_3 ... & R_{n-1} \\[0.3em]
       G_0 & G_1 & G_2 & G_3 ... & G_{n-1} \\[0.3em]
       B_0 & B_1 & B_2 & B_3 ... & B_{n-1} \\[0.3em]
\end{pmatrix}
\begin{pmatrix}
       R_0     & G_0     & B_0\\[0.3em]
       R_1     & G_1     & B_1\\[0.3em]
       R_2     & G_2     & B_2\\[0.3em]
       R_3     & G_3     & B_3\\[0.3em]
		.      & .       & .  \\[0.3em]
       R_{n-1} & G_{n-1} & B_{n-1}\\[0.3em]
\end{pmatrix}
=
\begin{pmatrix}
       R \\[0.3em]
       G \\[0.3em]
       B \\[0.3em]
\end{pmatrix}
\begin{pmatrix}
       R & G & B\\[0.3em]
\end{pmatrix}
\]
The time complexity is $O(N^2),  (C_2^N) = O(N^2/2)$.
\end{compactitem}

%----------------------------------------------------------------------------------------
